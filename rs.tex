% Last Modified 12/18/2013

\documentclass[11pt,oneside]{article}

\usepackage{geometry}
\usepackage{xspace}
\usepackage{url}
\usepackage{comment}

\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in,headheight=0in,headsep=0in,footskip=.3in}

\begin{document}

\title{\textbf{Research Statement}}
\author{\textbf{Xiang Cai}}
\date{}

\maketitle

\section{Introduction}

The Internet has become an important part in many people's everyday life, and it has also changed the way people used to live. We fetch news by browsing websites rather than reading the newspaper, chat with friends on social networks, and manage bank accounts online, etc. We embrace the great conveniences these services bring us, but at the same time, we have to face the inevitable problems that come with them -- network security and user privacy. There are many cyber attacks that can jeopardize online security and privacy. E.g, phishing websites can trick you to reveal your passwords, social engineering attacks may link your online account to your real life identity, etc.

One such attack is called a website fingerprinting attack \cite{hintz-pets02}, which enables an adversary to infer which website a victim is visiting, even if the victim uses an encrypting proxy, such as Tor \cite{tor-website}. Encrypting transferred data can hide the data itself, but still reveals other information, e.g. packet sizes, timing, and directions of packets, etc. In a website fingerprinting attack, an adversary analyzes these features, and attempts to infer the web page being visited by a victim. This attack scenario only requires an adversary to be able to eavesdrop, which is hard to be detected by a victim, yet can cause serious privacy issues. It has been shown that web page fingerprinting attacks are possible against many privacy services, including IPSec tunnels, SSH tunnels, and Tor. If mounted by internet censors, the attacks clearly can affect millions of users. Many researchers have stepped into the battlefield of developing website fingerprinting attacks and defenses. It is interesting to know that who will win this battle, and what are the prices the winner has to pay. E.g. whether a defender has to suffer a huge bandwidth cost to prevent information leakage, or an attack needs too much computing power than feasible to get what he wants. My research tries answer these questions by thoroughly analyzing this type of attack, and building real attack and defense systems to evaluate their impacts in reality.

\section{Previous and Ongoing Work}

Several researchers have developed web page fingerprinting attacks on encrypted web traffic, as occurs when the victim uses HTTPS, link-level encryption, such as WPA, or an encrypting tunnel such as SSH, a VPN, or IPSec. Herrmann, et al., used a Multinomial Naive Bayes classifier on features that captured only packet sizes, and achieved over 94\% success in recognizing packet traces from a set of 775 possible web pages visited using SSH \cite{herrmann-ccsw09}. Panchenko, et al., used ad hoc, HTTP-specific features with support vector machines to achieve a 54.61\% success rate on the same data set \cite{panchenko-wpes11}. On the other hand, Dyer et al. performed a thorough survey of past attacks and past network-level defenses and found that no network-level defense was secure \cite{dyer-snp12}. 

In order to build a secure and efficient defense system, we need to understand why existing attacks can achieve high success rates and where existing defenses fall short. After investigating existing attacks, we found that most attacks focus on packet sizes, and many throw away all information about packet ordering. Packet sizes do carry a lot of information in these scenarios, where
data packets are simply padded to a multiple of the block size (typically 16 bytes), but Tor pads all data packets to
a multiple of 512 bytes, providing much less information. However, the attacker can observe more things from the traffic other than just packet sizes, e.g. the order of packets exchanged between the user and the proxy. We believe a better attacker exists if he utilizes all what he observed.

% dlsvm
Web pages can consist of multiple objects, such as HTML files, images, and flash objects, and browsers send separate requests for each object. There is some inherent stability in the ordering of requests: browsers cannot request an object until they have received the portion of a page that references it. These facts suggest a simple representation for the attacker's traffic observations, and a similarity metric the attacker can use to compare traces.  We developed a web site fingerprinting attack DLSVM \cite{cai-ccs12}, which represents a trace of $\ell$ packets as a vector $t=(d_1, \ldots, d_\ell)$, where $d_i=\pm s_i$, where $s_i$ is the size of the $i$th packet and the sign indicates the direction of the packet.  Our attack compares traces $t$ and $t'$ using the Damerau-Levenshtein edit distance~\cite{navarro-acmcs01}, which is the length of the shortest sequence of character insertions, deletions, substitutions, and transpositions required to transform $t$ into $t'$. In the context of our packet traces, these edits correspond to packet and request re-ordering, request omissions (e.g. due to caching), and slight variations in the sizes of requests and responses. Thus, this model and distance metric are a good match for real network and HTTP-level behavior. To build a classifier for recognizing encrypted, anonymized page loads
of 1 of $n$ web pages, an attacker collects $k$ traces of each page,
using the same privacy system, e.g. Tor or an SSH proxy, in use by the
victim.  He then trains a support vector machine~\cite{vapnik-svm95}
using a kernel based on edit distance. Evaluation shows that our attack significantly outperforms other proposed attacks on these
and other defenses. Our attack can determine which web page, out of 100 possibilities, a victim is visiting with
over 80\% success rate.

% csbuflo


\section{Future Research}


\bibliographystyle{plain}
\bibliography{xiang}

\end{document}


